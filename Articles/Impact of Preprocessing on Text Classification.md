
https://www.sciencedirect.com/science/article/pii/S0306457313000964?via%3Dihub


Aim : To analyze the effect on text classification when 4 common preprocessing steps of tokenization, stop-word removal, lowercase conversion, and [[Stemming and Lemmatization| stemming]] are applied.

Datasets : E-mail and news, and in two different languages, namely Turkish and English.

Results:

Appropriate combinations of preprocessing tasks depending on the domain and language may provide a significant improvement on accuracy whereas inappropriate combinations may degrade the accuracy as well. But there is no unique combination of preprocessing tasks providing successful results for every domain and language studied on.