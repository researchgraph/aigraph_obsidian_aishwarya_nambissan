[How to Fine-Tune BERT for Text Classification? | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-030-32381-3_16)

#BERTopic #fine-tune #classification 

Further Pre-trainin:
- Within-Task Further Pre-training.
- In-domain and Cross-Domain Further Pre-training.

Experimental findings: 
1. With an appropriate layer-wise decreasing learning rate, BERT can overcome the catastrophic forgetting problem. 
2. Within-task and in-domain further pre-training can significantly boost its performance. 
3. A preceding multi-task fine-tuning is also helpful to the single-task fine-tuning,  
but its benefit is smaller than further pre-training.
4. BERT with further pre-training performs well in few-shot text classification.